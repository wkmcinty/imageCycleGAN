{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Numpy & Scipy imports\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.misc\n",
    "\n",
    "# Local imports\n",
    "import utils\n",
    "from data_loader import get_emoji_loader\n",
    "from models import CycleGenerator, DCDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 11\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "batchSize = 64\n",
    "workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                    transforms.Resize(image_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join('./emojis', \"Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(train_path, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batchSize,\n",
    "                                         shuffle=True, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nepochs = 100\n",
    "lr = 0.0002\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a transposed-convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False))\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True, init_zero_weights=False):\n",
    "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "    if init_zero_weights:\n",
    "        conv_layer.weight.data = torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.001\n",
    "    layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, conv_dim):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_layer = conv(in_channels=conv_dim, out_channels=conv_dim, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 3\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            # nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            # nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            # nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            # nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGenerator(nn.Module):\n",
    "    \"\"\"Defines the architecture of the generator network.\n",
    "       Note: Both generators G_XtoY and G_YtoX have the same architecture in this assignment.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_dim=64, init_zero_weights=False):\n",
    "        super(CycleGenerator, self).__init__()\n",
    "\n",
    "        ###########################################\n",
    "        ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n",
    "        ###########################################\n",
    "\n",
    "        # 1. Define the encoder part of the generator (that extracts features from the input image)\n",
    "        # self.conv1 = conv(...)\n",
    "        # self.conv2 = conv(...)\n",
    "        self.conv1 = conv(3, conv_dim, 4, init_zero_weights=init_zero_weights)\n",
    "        self.conv2 = conv(conv_dim, 2*conv_dim, 4)\n",
    "\n",
    "        # 2. Define the transformation part of the generator\n",
    "        # self.resnet_block = ...\n",
    "        self.resnet_block = ResnetBlock(2*conv_dim)\n",
    "\n",
    "        # 3. Define the decoder part of the generator (that builds up the output image from features)\n",
    "        # self.deconv1 = deconv(...)\n",
    "        # self.deconv2 = deconv(...)\n",
    "        self.deconv1 = deconv(2*conv_dim, conv_dim, 4)\n",
    "        self.deconv2 = deconv(conv_dim, 3, 4, batch_norm=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Generates an image conditioned on an input image.\n",
    "            Input\n",
    "            -----\n",
    "                x: BS x 3 x 32 x 32\n",
    "            Output\n",
    "            ------\n",
    "                out: BS x 3 x 32 x 32\n",
    "        \"\"\"\n",
    "\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "\n",
    "        out = F.relu(self.resnet_block(out))\n",
    "\n",
    "        out = F.relu(self.deconv1(out))\n",
    "        out = F.tanh(self.deconv2(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DCDiscriminator(nn.Module):\n",
    "    \"\"\"Defines the architecture of the discriminator network.\n",
    "       Note: Both discriminators D_X and D_Y have the same architecture in this assignment.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_dim=64):\n",
    "        super(DCDiscriminator, self).__init__()\n",
    "\n",
    "        ###########################################\n",
    "        ##   FILL THIS IN: CREATE ARCHITECTURE   ##\n",
    "        ###########################################\n",
    "\n",
    "        # self.conv1 = conv(...)\n",
    "        # self.conv2 = conv(...)\n",
    "        # self.conv3 = conv(...)\n",
    "        # self.conv4 = conv(...)\n",
    "        self.conv1 = conv(3, conv_dim, 4);\n",
    "        self.conv2 = conv(conv_dim, 2*conv_dim, 4)\n",
    "        self.conv3 = conv(2*conv_dim, 4*conv_dim, 4)\n",
    "        self.conv4 = conv(4*conv_dim, 1, 4,padding=1, batch_norm=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "\n",
    "        out = self.conv4(out).squeeze()\n",
    "        out = F.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = CycleGenerator()\n",
    "G.apply(weights_init)\n",
    "print(G)\n",
    "\n",
    "D = DCDiscriminator()\n",
    "D.apply(weights_init)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "D.cuda()\n",
    "G.cuda()\n",
    "criterion.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "# setup optimizers\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure for plotting\n",
    "num_test_samples = 16\n",
    "size_figure_grid = int(math.sqrt(num_test_samples))\n",
    "fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(6, 6))\n",
    "for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "    ax[i,j].get_xaxis().set_visible(False)\n",
    "    ax[i,j].get_yaxis().set_visible(False)\n",
    "\n",
    "def display_samples(fake_images):\n",
    "    for k in range(num_test_samples):\n",
    "        i = k//4\n",
    "        j = k%4\n",
    "        \n",
    "        img = fake_images[k].data.cpu() / 2 + 0.5\n",
    "        npimg = img.numpy()\n",
    "        \n",
    "        ax[i,j].cla()\n",
    "        ax[i,j].imshow(np.transpose(npimg, (1, 2, 0)), cmap='Greys')\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "\n",
    "display_every = 100\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        D.zero_grad()\n",
    "        real_images, _ = data\n",
    "        batch_size = real_images.size(0)\n",
    "        real_images = Variable(real_images.cuda())\n",
    "        ones_label = Variable(torch.ones(batch_size).float().cuda())\n",
    "\n",
    "        output = D(real_images)\n",
    "        D_real_loss = criterion(output, ones_label)\n",
    "        D_real_loss.backward()\n",
    "        D_x = output.data.mean()\n",
    "\n",
    "        # train with fake\n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = G(noisev)\n",
    "        zeros_label = Variable(torch.zeros(batch_size).float().cuda())\n",
    "        output = D(fake.detach())\n",
    "        D_fake_loss = criterion(output, zeros_label)\n",
    "        D_fake_loss.backward()\n",
    "        D_G_z1 = output.data.mean()\n",
    "        D_loss = D_real_loss + D_fake_loss\n",
    "        D_optimizer.step()\n",
    "        \n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        G.zero_grad()\n",
    "        ones_label = Variable(torch.ones(batch_size).float().cuda())\n",
    "        D_out = D(fake)\n",
    "        G_loss = criterion(D_out, ones_label)\n",
    "        G_loss.backward()\n",
    "        D_G_z2 = output.data.mean()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, nepochs, i, len(dataloader),\n",
    "                 D_loss.data[0], G_loss.data[0], D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        \n",
    "        if i % display_every == 0:\n",
    "            # DISPLAY GRID OF SAMPLES\n",
    "            test_images = G(fixed_noise)\n",
    "            display_samples(test_images)\n",
    "        \n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
