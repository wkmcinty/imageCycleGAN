{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Numpy & Scipy imports\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.misc\n",
    "\n",
    "# Local imports\n",
    "import utils\n",
    "from data_loader import get_emoji_loader\n",
    "from models import CycleGenerator, DCDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 11\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_models(G_XtoY, G_YtoX, D_X, D_Y):\n",
    "    \"\"\"Prints model information for the generators and discriminators.\n",
    "    \"\"\"\n",
    "    print(\"                 G_XtoY                \")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(G_XtoY)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"                 G_YtoX                \")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(G_YtoX)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"                  D_X                  \")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(D_X)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"                  D_Y                  \")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(D_Y)\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(opts):\n",
    "    \"\"\"Builds the generators and discriminators.\n",
    "    \"\"\"\n",
    "    G_XtoY = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights)\n",
    "    G_YtoX = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights)\n",
    "    D_X = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
    "    D_Y = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
    "\n",
    "    print_models(G_XtoY, G_YtoX, D_X, D_Y)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        G_XtoY.cuda()\n",
    "        G_YtoX.cuda()\n",
    "        D_X.cuda()\n",
    "        D_Y.cuda()\n",
    "        print('Models moved to GPU.')\n",
    "\n",
    "    return G_XtoY, G_YtoX, D_X, D_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, opts):\n",
    "    \"\"\"Saves the parameters of both generators G_YtoX, G_XtoY and discriminators D_X, D_Y.\n",
    "    \"\"\"\n",
    "    G_XtoY_path = os.path.join(opts.checkpoint_dir, 'G_XtoY.pkl')\n",
    "    G_YtoX_path = os.path.join(opts.checkpoint_dir, 'G_YtoX.pkl')\n",
    "    D_X_path = os.path.join(opts.checkpoint_dir, 'D_X.pkl')\n",
    "    D_Y_path = os.path.join(opts.checkpoint_dir, 'D_Y.pkl')\n",
    "    torch.save(G_XtoY.state_dict(), G_XtoY_path)\n",
    "    torch.save(G_YtoX.state_dict(), G_YtoX_path)\n",
    "    torch.save(D_X.state_dict(), D_X_path)\n",
    "    torch.save(D_Y.state_dict(), D_Y_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(opts):\n",
    "    \"\"\"Loads the generator and discriminator models from checkpoints.\n",
    "    \"\"\"\n",
    "    G_XtoY_path = os.path.join(opts.load, 'G_XtoY.pkl')\n",
    "    G_YtoX_path = os.path.join(opts.load, 'G_YtoX.pkl')\n",
    "    D_X_path = os.path.join(opts.load, 'D_X.pkl')\n",
    "    D_Y_path = os.path.join(opts.load, 'D_Y.pkl')\n",
    "\n",
    "    G_XtoY = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights)\n",
    "    G_YtoX = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights)\n",
    "    D_X = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
    "    D_Y = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
    "\n",
    "    G_XtoY.load_state_dict(torch.load(G_XtoY_path, map_location=lambda storage, loc: storage))\n",
    "    G_YtoX.load_state_dict(torch.load(G_YtoX_path, map_location=lambda storage, loc: storage))\n",
    "    D_X.load_state_dict(torch.load(D_X_path, map_location=lambda storage, loc: storage))\n",
    "    D_Y.load_state_dict(torch.load(D_Y_path, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        G_XtoY.cuda()\n",
    "        G_YtoX.cuda()\n",
    "        D_X.cuda()\n",
    "        D_Y.cuda()\n",
    "        print('Models moved to GPU.')\n",
    "\n",
    "    return G_XtoY, G_YtoX, D_X, D_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_images(sources, targets, opts, k=10):\n",
    "    \"\"\"Creates a grid consisting of pairs of columns, where the first column in\n",
    "    each pair contains images source images and the second column in each pair\n",
    "    contains images generated by the CycleGAN from the corresponding images in\n",
    "    the first column.\n",
    "    \"\"\"\n",
    "    _, _, h, w = sources.shape\n",
    "    row = int(np.sqrt(opts.batch_size))\n",
    "    merged = np.zeros([3, row*h, row*w*2])\n",
    "    for idx, (s, t) in enumerate(zip(sources, targets)):\n",
    "        i = idx // row\n",
    "        j = idx % row\n",
    "        merged[:, i*h:(i+1)*h, (j*2)*h:(j*2+1)*h] = s\n",
    "        merged[:, i*h:(i+1)*h, (j*2+1)*h:(j*2+2)*h] = t\n",
    "    return merged.transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts):\n",
    "    \"\"\"Saves samples from both generators X->Y and Y->X.\n",
    "    \"\"\"\n",
    "    fake_X = G_YtoX(fixed_Y)\n",
    "    fake_Y = G_XtoY(fixed_X)\n",
    "\n",
    "    X, fake_X = utils.to_data(fixed_X), utils.to_data(fake_X)\n",
    "    Y, fake_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y)\n",
    "\n",
    "    merged = merge_images(X, fake_Y, opts)\n",
    "    path = os.path.join(opts.sample_dir, 'sample-{:06d}-X-Y.png'.format(iteration))\n",
    "    scipy.misc.imsave(path, merged)\n",
    "    print('Saved {}'.format(path))\n",
    "\n",
    "    merged = merge_images(Y, fake_X, opts)\n",
    "    path = os.path.join(opts.sample_dir, 'sample-{:06d}-Y-X.png'.format(iteration))\n",
    "    scipy.misc.imsave(path, merged)\n",
    "    print('Saved {}'.format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, opts):\n",
    "    \"\"\"Runs the training loop.\n",
    "        * Saves checkpoint every opts.checkpoint_every iterations\n",
    "        * Saves generated samples every opts.sample_every iterations\n",
    "    \"\"\"\n",
    "\n",
    "    # Create generators and discriminators\n",
    "    if opts.load:\n",
    "        G_XtoY, G_YtoX, D_X, D_Y = load_checkpoint(opts)\n",
    "    else:\n",
    "        G_XtoY, G_YtoX, D_X, D_Y = create_model(opts)\n",
    "\n",
    "    g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters\n",
    "    d_params = list(D_X.parameters()) + list(D_Y.parameters())  # Get discriminator parameters\n",
    "\n",
    "    # Create optimizers for the generators and discriminators\n",
    "    g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n",
    "    d_optimizer = optim.Adam(d_params, opts.lr, [opts.beta1, opts.beta2])\n",
    "\n",
    "    iter_X = iter(dataloader_X)\n",
    "    iter_Y = iter(dataloader_Y)\n",
    "\n",
    "    test_iter_X = iter(test_dataloader_X)\n",
    "    test_iter_Y = iter(test_dataloader_Y)\n",
    "\n",
    "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
    "    # constant throughout training, that allow us to inspect the model's performance.\n",
    "    fixed_X = utils.to_var(test_iter_X.next()[0])\n",
    "    fixed_Y = utils.to_var(test_iter_Y.next()[0])\n",
    "\n",
    "    iter_per_epoch = min(len(iter_X), len(iter_Y))\n",
    "\n",
    "    for iteration in range(1, opts.train_iters+1):\n",
    "\n",
    "        # Reset data_iter for each epoch\n",
    "        if iteration % iter_per_epoch == 0:\n",
    "            iter_X = iter(dataloader_X)\n",
    "            iter_Y = iter(dataloader_Y)\n",
    "\n",
    "        images_X, labels_X = iter_X.next()\n",
    "        images_X, labels_X = utils.to_var(images_X), utils.to_var(labels_X).long().squeeze()\n",
    "\n",
    "        images_Y, labels_Y = iter_Y.next()\n",
    "        images_Y, labels_Y = utils.to_var(images_Y), utils.to_var(labels_Y).long().squeeze()\n",
    "\n",
    "\n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATORS\n",
    "        # ============================================\n",
    "\n",
    "        #########################################\n",
    "        ##             FILL THIS IN            ##\n",
    "        #########################################\n",
    "\n",
    "        # Train with real images\n",
    "        d_optimizer.zero_grad()\n",
    "\n",
    "        # 1. Compute the discriminator losses on real images\n",
    "        D_X_loss = torch.mean((D_X.forward(images_X)-1)**2)\n",
    "        D_Y_loss = torch.mean((D_Y.forward(images_Y)-1)**2)\n",
    "\n",
    "        d_real_loss = D_X_loss + D_Y_loss\n",
    "        d_real_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train with fake images\n",
    "        d_optimizer.zero_grad()\n",
    "\n",
    "        # 2. Generate fake images that look like domain X based on real images in domain Y\n",
    "        fake_X = G_YtoX.forward(images_Y)\n",
    "\n",
    "        # 3. Compute the loss for D_X\n",
    "        D_X_loss = torch.mean((D_X.forward(fake_X))**2)\n",
    "\n",
    "        # 4. Generate fake images that look like domain Y based on real images in domain X\n",
    "        fake_Y = G_XtoY.forward(images_X)\n",
    "\n",
    "        # 5. Compute the loss for D_Y\n",
    "        D_Y_loss = torch.mean((D_Y.forward(fake_Y))**2)\n",
    "\n",
    "        d_fake_loss = D_X_loss + D_Y_loss\n",
    "        d_fake_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATORS\n",
    "        # =========================================\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        ##    FILL THIS IN: Y--X-->Y CYCLE     ##\n",
    "        #########################################\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        # 1. Generate fake images that look like domain X based on real images in domain Y\n",
    "        fake_X = G_YtoX.forward(images_Y)\n",
    "\n",
    "        # 2. Compute the generator loss based on domain X\n",
    "        g_loss = torch.mean((D_X.forward(fake_X)-1)**2)\n",
    "\n",
    "\n",
    "        if opts.use_cycle_consistency_loss:\n",
    "            reconstructed_Y = G_XtoY(fake_X)\n",
    "            # 3. Compute the cycle consistency loss (the reconstruction loss)\n",
    "            # cycle_consistency_loss = ...\n",
    "            g_loss += cycle_consistency_loss\n",
    "\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        ##    FILL THIS IN: X--Y-->X CYCLE     ##\n",
    "        #########################################\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        # 1. Generate fake images that look like domain Y based on real images in domain X\n",
    "        fake_Y = G_XtoY.forward(images_X)\n",
    "\n",
    "        # 2. Compute the generator loss based on domain Y\n",
    "        g_loss = torch.mean((D_Y.forward(fake_Y)-1)**2)\n",
    "\n",
    "        if opts.use_cycle_consistency_loss:\n",
    "            reconstructed_X = G_YtoX(fake_Y)\n",
    "            # 3. Compute the cycle consistency loss (the reconstruction loss)\n",
    "            # cycle_consistency_loss = ...\n",
    "            g_loss += cycle_consistency_loss\n",
    "\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "\n",
    "        # Print the log info\n",
    "        if iteration % opts.log_step == 0:\n",
    "            print('Iteration [{:5d}/{:5d}] | d_real_loss: {:6.4f} | d_Y_loss: {:6.4f} | d_X_loss: {:6.4f} | '\n",
    "                  'd_fake_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                    iteration, opts.train_iters, d_real_loss.data[0], D_Y_loss.data[0],\n",
    "                    D_X_loss.data[0], d_fake_loss.data[0], g_loss.data[0]))\n",
    "\n",
    "\n",
    "        # Save the generated samples\n",
    "        if iteration % opts.sample_every == 0:\n",
    "            save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts)\n",
    "\n",
    "\n",
    "        # Save the model parameters\n",
    "        if iteration % opts.checkpoint_every == 0:\n",
    "            checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(opts):\n",
    "    \"\"\"Loads the data, creates checkpoint and sample directories, and starts the training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create train and test dataloaders for images from the two domains X and Y\n",
    "    dataloader_X, test_dataloader_X = get_emoji_loader(emoji_type=opts.X, opts=opts)\n",
    "    dataloader_Y, test_dataloader_Y = get_emoji_loader(emoji_type=opts.Y, opts=opts)\n",
    "\n",
    "    # Create checkpoint and sample directories\n",
    "    utils.create_dir(opts.checkpoint_dir)\n",
    "    utils.create_dir(opts.sample_dir)\n",
    "\n",
    "    # Start training\n",
    "    training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_opts(opts):\n",
    "    \"\"\"Prints the values of all command-line arguments.\n",
    "    \"\"\"\n",
    "    print('=' * 80)\n",
    "    print('Opts'.center(80))\n",
    "    print('-' * 80)\n",
    "    for key in opts.__dict__:\n",
    "        if opts.__dict__[key]:\n",
    "            print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser():\n",
    "    \"\"\"Creates a parser for command-line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Model hyper-parameters\n",
    "    parser.add_argument('--image_size', type=int, default=32, help='The side length N to convert images to NxN.')\n",
    "    parser.add_argument('--g_conv_dim', type=int, default=32)\n",
    "    parser.add_argument('--d_conv_dim', type=int, default=32)\n",
    "    parser.add_argument('--use_cycle_consistency_loss', action='store_true', default=False, help='Choose whether to include the cycle consistency term in the loss.')\n",
    "    parser.add_argument('--init_zero_weights', action='store_true', default=False, help='Choose whether to initialize the generator conv weights to 0 (implements the identity function).')\n",
    "\n",
    "    # Training hyper-parameters\n",
    "    parser.add_argument('--train_iters', type=int, default=600, help='The number of training iterations to run (you can Ctrl-C out earlier if you want).')\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='The number of images in a batch.')\n",
    "    parser.add_argument('--num_workers', type=int, default=0, help='The number of threads to use for the DataLoader.')\n",
    "    parser.add_argument('--lr', type=float, default=0.0003, help='The learning rate (default 0.0003)')\n",
    "    parser.add_argument('--beta1', type=float, default=0.5)\n",
    "    parser.add_argument('--beta2', type=float, default=0.999)\n",
    "\n",
    "    # Data sources\n",
    "    parser.add_argument('--X', type=str, default='Apple', choices=['Apple', 'Windows'], help='Choose the type of images for domain X.')\n",
    "    parser.add_argument('--Y', type=str, default='Windows', choices=['Apple', 'Windows'], help='Choose the type of images for domain Y.')\n",
    "\n",
    "    # Saving directories and checkpoint/sample iterations\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints_cyclegan')\n",
    "    parser.add_argument('--sample_dir', type=str, default='samples_cyclegan')\n",
    "    parser.add_argument('--load', type=str, default=None)\n",
    "    parser.add_argument('--log_step', type=int , default=10)\n",
    "    parser.add_argument('--sample_every', type=int , default=100)\n",
    "    parser.add_argument('--checkpoint_every', type=int , default=800)\n",
    "\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = create_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--image_size IMAGE_SIZE]\n",
      "                             [--g_conv_dim G_CONV_DIM]\n",
      "                             [--d_conv_dim D_CONV_DIM]\n",
      "                             [--use_cycle_consistency_loss]\n",
      "                             [--init_zero_weights] [--train_iters TRAIN_ITERS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--num_workers NUM_WORKERS] [--lr LR]\n",
      "                             [--beta1 BETA1] [--beta2 BETA2]\n",
      "                             [--X {Apple,Windows}] [--Y {Apple,Windows}]\n",
      "                             [--checkpoint_dir CHECKPOINT_DIR]\n",
      "                             [--sample_dir SAMPLE_DIR] [--load LOAD]\n",
      "                             [--log_step LOG_STEP]\n",
      "                             [--sample_every SAMPLE_EVERY]\n",
      "                             [--checkpoint_every CHECKPOINT_EVERY]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/3120462/jupyter/kernel-9f07102c-25eb-41e9-b659-d365ec66d299.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "opts = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f51c986d0087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1730\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2383\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2385\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2371\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2372\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
